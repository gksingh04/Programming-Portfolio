{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dogri_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cfa52e314ebb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdogri_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dogri_dict'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import re\n",
    "import dogri_dict\n",
    "import string\n",
    "import math\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "def dogriStopWords(dogri_dict):\n",
    "    return(dogri_dict)\n",
    "\n",
    "def wordListToProbDict(wordlist):\n",
    "    wordprob = [round((wordlist.count(p)/len(wordlist)),4) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordprob))\n",
    "    \n",
    "def wordListToVarDict(wordlist, mean):\n",
    "    wordvar=[round((math.pow((wordlist.count(p)-mean),2)/len(wordlist)),4) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordvar))\n",
    "\n",
    "def getUniqueWords(allWords):\n",
    "    uniqueWords = [] \n",
    "    for i in allWords:\n",
    "        if not i in uniqueWords:\n",
    "            uniqueWords.append(i)\n",
    "    return uniqueWords\n",
    "\n",
    "with open (\"E:\\\\phd items\\\\final\\\\full1.txt\", encoding=\"UTF-8\", errors='ignore') as in_file:  # Open file full1.txt for reading of text data.\n",
    "  contents = in_file.read()# Read the entire file into a variable named contents.\n",
    "#print(contents)\n",
    "words=[]\n",
    "with open (\"E:\\\\phd items\\\\final\\\\dogri_ner.txt\", encoding=\"UTF-8\", errors='ignore') as in_file:  # Open file lorem.txt for reading of text data.\n",
    "  ner = in_file.read()# Read the entire file into a variable named ner.\n",
    "nerwords=ner.split()     #list of ner words\n",
    "print(len(nerwords))\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~|\\n'''\n",
    "no_punct = \"\"\n",
    "for char in contents:\n",
    "    if char not in punctuations:\n",
    "        no_punct = no_punct + char\n",
    "   \n",
    "# display the unpunctuated string\n",
    "#print(no_punct)\n",
    "no_punct.strip()\n",
    "sentences=no_punct.split('।')      #all the sentences in the corpus\n",
    "print(len(sentences))\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens=sentence.split()\n",
    "    words=words+tokens      #all the words in the corpus\n",
    "print(len(words))\n",
    "un_words=getUniqueWords(words)\n",
    "#print(un_words)\n",
    "print(len(un_words))\n",
    "\n",
    "fdist_dogri=FreqDist(words)\n",
    "dogridict=dogri_dict.wordListToFreqDict(words)\n",
    "print(\"Dogri Dictionary\")\n",
    "for x in dogridict:\n",
    "    print(x,\":\",dogridict[x])\n",
    "meandogri=np.array(list(dogridict.values())).mean()\n",
    "print(meandogri)\n",
    "print(\"Probability of Dogri Dictionary\")\n",
    "probdogridict=wordListToProbDict(words)\n",
    "for y in probdogridict:\n",
    "    print(y,\":\",probdogridict[y])\n",
    "print(\"Variance of Dogri Dictionary\")\n",
    "vardogridict=wordListToVarDict(words,meandogri)\n",
    "for z in vardogridict:\n",
    "    print(z,\":\",vardogridict[z])\n",
    "\n",
    "pdist=dogri_dict.sortFreqDict(probdogridict)\n",
    "#print(pdist)\n",
    "vdist=dogri_dict.sortFreqDict(vardogridict)\n",
    "#print(vdist)\n",
    "li1=words\n",
    "li2=nerwords\n",
    "final_words = []\n",
    "for x in li1:\n",
    "    if x not in li2:\n",
    "        final_words.append(x)\n",
    "print(len(final_words))\n",
    "\n",
    "fdist=FreqDist(final_words)\n",
    "#for w in fdist:\n",
    "    #print(w, '->', fdist[w], ';',)\n",
    "dogri_stopwords=fdist.most_common(138)\n",
    "\n",
    "fdist.plot(100)\n",
    "#dogri_dict.plot_freqdist_freq(fdist.most_common(100))\n",
    "dogri_stopword_dict=dogri_dict.wordListToFreqDict(dogri_stopwords)\n",
    "#print(dogri_stopword_dict)\n",
    "print(dogriStopWords(dogri_stopword_dict))\n",
    "dogri_ner_dict=dogri_dict.wordListToFreqDict(nerwords)\n",
    "#print(dogri_ner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Shivangi\\\\Desktop\\\\research\\\\dataset\\\\news1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0fa0d28461b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\Shivangi\\\\Desktop\\\\research\\\\dataset\\\\news1.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"UTF-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Open file lorem.txt for reading of text data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Read the entire file into a variable named contents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Shivangi\\\\Desktop\\\\research\\\\dataset\\\\news1.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "#from . import dogri_dict\n",
    "\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "\n",
    "with open (\"C:\\\\Users\\\\Shivangi\\\\Desktop\\\\research\\\\dataset\\\\news1.txt\", encoding=\"UTF-8\", errors='ignore') as in_file:  \n",
    "    # Open file lorem.txt for reading of text data.\n",
    "  contents = in_file.read()# Read the entire file into a variable named contents.\n",
    "#print(contents)\n",
    "#words=[]\n",
    "punctuations = '''!()[]{};:'\"\\,<>./?@#$%^&*_~|-\\n'''\n",
    "no_punct = \"\"\n",
    "for char in contents:\n",
    "   if char not in punctuations:\n",
    "       no_punct = no_punct + char\n",
    "\n",
    "# display the unpunctuated string\n",
    "\n",
    "no_punct.strip()\n",
    "print(no_punct)\n",
    "sentence=str(no_punct.split('?'))\n",
    "sentences=str(sentence.split('।'))\n",
    "\n",
    "               #print(sentences)\n",
    "print(len(sentences))\n",
    "w=word_tokenize(sentences)\n",
    "def generate_stem_words(word):\n",
    "        suffixes = {\n",
    "    1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
    "    2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
    "    3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
    "    4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
    "    5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
    "}\n",
    "        for L in 5, 4, 3, 2, 1:\n",
    "            if len(word) > L + 1:\n",
    "                for suf in suffixes[L]:\n",
    "                    #print(type(suf),type(word),word,suf)\n",
    "                    if word.endswith(suf):\n",
    "                        #print 'h'\n",
    "                        return word[:-L]\n",
    "        return word\n",
    "\n",
    "s = []\n",
    "s = ['प्रदेश', 'च', 'स्थितियां', 'काबू', 'थमां', 'काबू']\n",
    "l = []\n",
    "l = ['प्रदेश', 'च', 'स्थिति', 'काबू', 'थमां', 'काबू']\n",
    "\n",
    "for i in range(len(l)):\n",
    "    if s[i] == l[i]:\n",
    "        print(s[i])\n",
    "    else:\n",
    "        print(l[i])\n",
    "\n",
    "w\n",
    "#print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
